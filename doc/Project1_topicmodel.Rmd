---
title: "Project 1"
output: html_notebook
---

# 0 Load packages and functions. 
```{r, message=FALSE, warning=FALSE}
packages.used=c("rvest", "tibble", "qdap",
                "sentimentr", "gplots", "dplyr",
                "syuzhet", "factoextra","beeswarm",
                "scales", "RColorBrewer","RANN",
                "tm", "topicmodels","wordcloud",
                "tidytext","readxl")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

# packages
library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("topicmodels")
library("wordcloud")
library("tidytext")
library("readxl")

# functions
source("~/Documents/GitHub/Spring2018-Project1-xueyao0917/lib/speechFuncs.R")
```

# 1 Data harvest
## 1.1 
```{r, message=FALSE, warning=FALSE}
speech.list <- read_xlsx("~/Documents/GitHub/Spring2018-Project1-xueyao0917/data/InaugurationInfo.xlsx")
speech.list$twoTerm <- "N"
twoTermPresident <- speech.list[speech.list$Term == 2,]$President
speech.list[is.element(speech.list$President,twoTermPresident),]$twoTerm <- "Y"
speech.list[58,]$twoTerm <- "T"
```

## 1.2
```{r}
setwd("~/Documents/GitHub/Spring2018-Project1-xueyao0917/data/InauguralSpeeches")
speech.list$fullName <- paste(speech.list$File, speech.list$Term, sep = "-")
speech.list$fullName <- paste("inaug", speech.list$fullName, ".txt", sep = "")
speech.list$text <- NA
for(i in seq(nrow(speech.list))) {
  speech.list$text[i] <- readLines(speech.list$fullName[i])
}
speech.list[58,]$Words <- word_count(speech.list[58,]$text)
```

# 2 World Cloud
## 2.1 Texting Process
For the speeches, we remove extra white space, convert all letters to the lower case, remove stop words, removed empty words due to formatting errors, and remove punctuation.
```{r, message=FALSE, warning=FALSE, fig.height=6, fig.width=6}
par(mfrow=c(1,3))
folder.path="~/Documents/GitHub/Spring2018-Project1-xueyao0917/data/InauguralSpeeches"
ff.all<-Corpus(DirSource(folder.path))
ff.all<-tm_map(ff.all, stripWhitespace)
ff.all<-tm_map(ff.all, content_transformer(tolower))
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))
ff.all<-tm_map(ff.all, removeWords, character(0))
ff.all<-tm_map(ff.all, removePunctuation)
mystopwords <- c("will","would","may","shall","can",
                 "must","ever","never","yet","every",
                 "many","one","upon","now","new",
                 "thank","make","let")
ff.all <- tm_map(ff.all, removeWords, mystopwords)
```

##  2.2 Word Cloud
```{r}
speech.list.sorted <- speech.list[order(speech.list$File),]
ff.two <- ff.all[speech.list.sorted$twoTerm == "Y"]
ff.one <- ff.all[speech.list.sorted$twoTerm == "N"]
ff.trump <- ff.all[speech.list.sorted$twoTerm == "T"]

#Two Terms
tdm.two<-TermDocumentMatrix(ff.two)
tdm.tidy.two=tidy(tdm.two)
tdm.overall.two=summarise(group_by(tdm.tidy.two, term), sum(count))
wordcloud(tdm.overall.two$term, tdm.overall.two$`sum(count)`,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.1,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Greens"))
#One Term
tdm.one<-TermDocumentMatrix(ff.one)
tdm.tidy.one=tidy(tdm.one)
tdm.overall.one=summarise(group_by(tdm.tidy.one, term), sum(count))
wordcloud(tdm.overall.one$term, tdm.overall.one$`sum(count)`,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.1,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Reds"))

#Trump
tdm.trump<-TermDocumentMatrix(ff.trump)
tdm.tidy.trump=tidy(tdm.trump)
tdm.overall.trump=summarise(group_by(tdm.tidy.trump, term), sum(count))
wordcloud(tdm.overall.trump$term, tdm.overall.trump$`sum(count)`,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.1,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues"))
```

# 3 Length of Sentences
## 3.1
We will use sentences as units of analysis for this project, as sentences are natural languge units for organizing thoughts and ideas. For each extracted sentence, we apply sentiment analysis using [NRC sentiment lexion](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm). "The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). The annotations were manually done by crowdsourcing."

We assign an sequential id to each sentence in a speech (`sent.id`) and also calculated the number of words in each sentence as *sentence length* (`word.count`).
```{r}
sentence.list=NULL
for(i in 1:nrow(speech.list)){
  sentences=sent_detect(speech.list$text[i],
                        endmarks = c("?", ".", "!", "|",";"))
  if(length(sentences)>0){
    emotions=get_nrc_sentiment(sentences)
    word.count=word_count(sentences)
    emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
    sentence.list=rbind(sentence.list, 
                        cbind(speech.list[i,-ncol(speech.list)],
                              sentences=as.character(sentences), 
                              word.count,
                              emotions,
                              sent.id=1:length(sentences)
                              )
    )
  }
}
```

Some non-sentences exist in raw data due to erroneous extra end-of sentence marks. Save sentence.list in the output folder.
```{r}
sentence.list=
  sentence.list%>%
  filter(!is.na(word.count))
write.csv(sentence.list, "../output/sentenceList.csv")
```

## 3.2 length of sentences
```{r}
#Two Terms
sentence.list.two=sentence.list%>%filter(twoTerm=="Y", Term==1) #the first speech
sentence.list.two$File=factor(sentence.list.two$File)
sentence.list.two$FileOrdered=reorder(sentence.list.two$File, 
                                  sentence.list.two$word.count, 
                                  mean, 
                                  order=T)
beeswarm(word.count~FileOrdered, 
         data=sentence.list.two,
         horizontal = TRUE,
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.5, cex.axis=0.5, cex.lab=0.8,
         spacing=5/nlevels(sentence.list.two$FileOrdered),
         las=2, ylab="", xlab="Number of words in a sentence.",
         main="Length of Sentences for Two Terms")

#One Term
sentence.list.one=sentence.list%>%filter(twoTerm=="N")
sentence.list.one$File=factor(sentence.list.one$File)
sentence.list.one$FileOrdered=reorder(sentence.list.one$File, 
                                  sentence.list.one$word.count, 
                                  mean, 
                                  order=T)
beeswarm(word.count~FileOrdered, 
         data=sentence.list.one,
         horizontal = TRUE,
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.5, cex.axis=0.5, cex.lab=0.8,
         spacing=5/nlevels(sentence.list.one$FileOrdered),
         las=2, ylab="", xlab="Number of words in a sentence.",
         main="Length of Sentences for One Term")


#Trump
sentence.list.trump=sentence.list%>%filter(twoTerm=="T")
sentence.list.trump$File=factor(sentence.list.trump$File)
sentence.list.trump$FileOrdered=reorder(sentence.list.trump$File, 
                                  sentence.list.trump$word.count, 
                                  mean, 
                                  order=T)
beeswarm(word.count~FileOrdered, 
         data=sentence.list.trump,
         horizontal = TRUE,
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.5, cex.axis=0.5, cex.lab=0.8,
         spacing=5/nlevels(sentence.list.trump$FileOrdered),
         las=2, ylab="", xlab="Number of words in a sentence.",
         main="Length of Sentences for Trump")
```

# 4 Sentiment Analsis
## 4.1 Sentence length variation over the course of the speech, with emotions. 
How our presidents (or candidates) alternate between long and short sentences and how they shift between different sentiments in their speeches. It is interesting to note that some presidential candidates' speech are more colorful than others.
```{r}
f.plotsent.len(In.list=sentence.list, IntwoTerm="Y")
f.plotsent.len(In.list=sentence.list, IntwoTerm="N")
f.plotsent.len(In.list=sentence.list, IntwoTerm="T" )
```

## 4.2 What are the emotionally changed sentences?
```{r}
print("Emotionally Changed Sentences for Two Terms")
speech.df=tbl_df(sentence.list)%>%
  filter(twoTerm=="Y", word.count>=3)%>%
  select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])

print("Emotionally Changed Sentences for One Term")
speech.df=tbl_df(sentence.list)%>%
  filter(twoTerm=="N", word.count>=3)%>%
  select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])

print("Emotionally Changed Sentences for Trump")
speech.df=tbl_df(sentence.list)%>%
  filter(twoTerm=="T", word.count>=3)%>%
  select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
```

## 4.3 Clustering of Emotions
### 4.3.1 Heatmap
```{r}
#Two Terms
heatmap.2(cor(sentence.list%>%filter(twoTerm=="Y")%>%select(anger:trust)), 
          scale = "none", 
          col = bluered(100), , margin=c(6, 6), key=F,
          trace = "none", density.info = "none")

emo.means=colMeans(select(sentence.list%>%filter(twoTerm=="Y"), anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1", 
            "chartreuse3", "blueviolet",
            "darkgoldenrod2", "dodgerblue3", 
            "darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Inaugural Speeches")

#One Terms
heatmap.2(cor(sentence.list%>%filter(twoTerm=="N")%>%select(anger:trust)), 
          scale = "none", 
          col = bluered(100), , margin=c(6, 6), key=F,
          trace = "none", density.info = "none")

emo.means=colMeans(select(sentence.list%>%filter(twoTerm=="N"), anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1", 
            "chartreuse3", "blueviolet",
            "darkgoldenrod2", "dodgerblue3", 
            "darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Inaugural Speeches")

#Trump
heatmap.2(cor(sentence.list%>%filter(twoTerm=="T")%>%select(anger:trust)), 
          scale = "none", 
          col = bluered(100), , margin=c(6, 6), key=F,
          trace = "none", density.info = "none")

emo.means=colMeans(select(sentence.list%>%filter(twoTerm=="T"), anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1", 
            "chartreuse3", "blueviolet",
            "darkgoldenrod2", "dodgerblue3", 
            "darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Inaugural Speeches")
```

### 4.3.2 Clustering
```{r}
presid.summary=tbl_df(sentence.list)%>%
  group_by(File)%>%
  summarise(
    anger=mean(anger),
    anticipation=mean(anticipation),
    disgust=mean(disgust),
    fear=mean(fear),
    joy=mean(joy),
    sadness=mean(sadness),
    surprise=mean(surprise),
    trust=mean(trust)
    #negative=mean(negative),
    #positive=mean(positive)
  )

presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(presid.summary[,-1], iter.max=200, 5)
fviz_cluster(km.res, 
             stand=F, repel= TRUE,
             data = presid.summary[,-1], xlab="", xaxt="n",
             show.clust.cent=FALSE)
```

# 5 Topic Modeling
## 5.1 Snipet
For topic modeling, we prepare a corpus of sentence snipets as follows. For each speech, we start with sentences and prepare a snipet with a given sentence with the flanking sentences. 
```{r}
corpus.list=sentence.list[2:(nrow(sentence.list)-1), ]
sentence.pre=sentence.list$sentences[1:(nrow(sentence.list)-2)]
sentence.post=sentence.list$sentences[3:(nrow(sentence.list)-1)]
corpus.list$snipets=paste(sentence.pre, corpus.list$sentences, sentence.post, sep=" ")
rm.rows=(1:nrow(corpus.list))[corpus.list$sent.id==1]
rm.rows=c(rm.rows, rm.rows-1)
corpus.list=corpus.list[-rm.rows, ]
```

## 5.2 Text processing
```{r}
docs <- Corpus(VectorSource(corpus.list$snipets))
#writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#Remove potentially problematic symbols
docs <-tm_map(docs,content_transformer(tolower))
#writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#Remove punctuation
docs <- tm_map(docs, removePunctuation)
#writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#Strip digits
docs <- tm_map(docs, removeNumbers)
#writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#Remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
#writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#Remove whitespace
docs <- tm_map(docs, stripWhitespace)
#writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#Stem document
docs <- tm_map(docs,stemDocument)
#writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))
```

## 5.3 Document-term Matrices. 
```{r}
dtm <- DocumentTermMatrix(docs)
#Convert rownames to filenames
rownames(dtm) <- paste(corpus.list$type, corpus.list$File,
                       corpus.list$Term, corpus.list$sent.id, sep="_")

rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document

dtm  <- dtm[rowTotals> 0, ]
corpus.list=corpus.list[rowTotals>0, ]
```

## 5.4 LDA
```{r}
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE

#Number of topics
k <- 15

#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter = iter, 
                                                 thin=thin))
#Docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
write.csv(ldaOut.topics,file="../output/DocsToTopics.csv")

#Top 6 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,20))
write.csv(ldaOut.terms,file="../output/TopicsToTerms.csv")

#Probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file="../output/TopicProbabilities.csv")
```

```{r}
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
  topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
ldaOut.terms
```

Based on the most popular terms and the most salient terms for each topic, we assign a hashtag to each topic. This part require manual setup as the topics are likely to change.
```{r}
topics.hash=c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O")
corpus.list$ldatopic=as.vector(ldaOut.topics)
corpus.list$ldahash=topics.hash[ldaOut.topics]

colnames(topicProbabilities)=topics.hash
corpus.list.df=cbind(corpus.list, topicProbabilities)
```

### 5.4.1 Heatmap
```{r, fig.width=3, fig.height=3}
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
              filter(twoTerm%in%c("Y","N","T"))%>%
              select(File, A:O)%>%
              group_by(File)%>%
              summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
rownames(topic.summary)=topic.summary[,1]

topic.plot=c(1, 13, 9, 11, 8, 3, 7)
print(topics.hash[topic.plot])

heatmap.2(as.matrix(topic.summary[,topic.plot+1]), 
          scale = "column", key=F, 
          col = bluered(100),
          cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
          trace = "none", density.info = "none")
```

### 5.4.2 
```{r}
par(mfrow=c(5, 1), mar=c(1,1,2,0), bty="n", xaxt="n", yaxt="n")

topic.plot=c(1, 13, 14, 15, 8, 9, 12)
print(topics.hash[topic.plot])

speech.df=tbl_df(corpus.list.df)%>%filter(twoTerm=="Y")%>%select(sent.id, A:O)
speech.df=as.matrix(speech.df)
speech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)
#speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
plot.stacked(speech.df[,1], speech.df[,topic.plot+1],
             xlab="Sentences", ylab="Topic share", main="Two Terms")

speech.df=tbl_df(corpus.list.df)%>%filter(twoTerm=="N")%>%select(sent.id, A:O)
speech.df=as.matrix(speech.df)
speech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)
#speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
plot.stacked(speech.df[,1], speech.df[,topic.plot+1],
             xlab="Sentences", ylab="Topic share", main="One Term")

speech.df=tbl_df(corpus.list.df)%>%filter(twoTerm=="T")%>%select(sent.id, A:O)
speech.df=as.matrix(speech.df)
speech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)
#speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
plot.stacked(speech.df[,1], speech.df[,topic.plot+1],
             xlab="Sentences", ylab="Topic share", main="Trump")
```

### 5.4.3
```{r}
speech.df=tbl_df(corpus.list.df)%>%filter(word.count<20)%>%select(sentences,A:O)
as.character(speech.df$sentences[apply(as.data.frame(speech.df[,-1]), 2, which.max)])
names(speech.df)[-1]
```

### 5.4.4 Clusting
```{r}
presid.summary=tbl_df(corpus.list.df)%>%
  select(File, A:O)%>%
  group_by(File)%>%
  summarise_each(funs(mean))

presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(scale(presid.summary[,-1]), iter.max=200, 5)
fviz_cluster(km.res, 
             stand=T, repel= TRUE,
             data = presid.summary[,-1],
             show.clust.cent=FALSE)
```