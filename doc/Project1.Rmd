---
title: "Project 1: Will Donald Trump serve a second term?"
output: html_notebook
---

# Introduction

In this project, we want to explore whether there is a relationship between presidents who served twos terms and their inaugural speeches. We compared one-term and two-term presidents' inaugural speeches in words, sentences, emotions and topics. In addition, we compared these two groups with Trump, and try to predict that will Donald Trump serve a second term? 

# 0 Check and install needed packages. Load the libraries and functions.
```{r, message=FALSE, warning=FALSE}
packages.used=c("rvest", "tibble", "qdap",
                "sentimentr", "gplots", "dplyr",
                "syuzhet", "factoextra","beeswarm",
                "scales", "RColorBrewer","RANN",
                "tm", "topicmodels","wordcloud",
                "tidytext","readxl")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

# packages
library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("topicmodels")
library("wordcloud")
library("tidytext")
library("readxl")

# functions
source("~/Documents/GitHub/Spring2018-Project1-xueyao0917/lib/speechFuncs.R")
```

# 1 Data harvest
## 1.1 Import speech list
We imported the xlsx file as speech list (`speech.list`) and divided president into two groups. Except Trump, there were 17 presidents who served two or more terms and 21 presidents who only served on term. Here was the list of two-term presidents.
```{r}
speech.list <- read_xlsx("~/Documents/GitHub/Spring2018-Project1-xueyao0917/data/InaugurationInfo.xlsx")
speech.list$twoTerm <- "N"
twoTermPresident <- speech.list[speech.list$Term == 2,]$President
print(twoTermPresident)
speech.list[is.element(speech.list$President,twoTermPresident),]$twoTerm <- "Y"
speech.list[25,]$twoTerm <- "Y" #Grover Cleveland - I
speech.list[58,]$twoTerm <- "T" #Trump
table(speech.list$twoTerm)
```

## 1.2
We imported the text from the txt file and stored them in ('text').
```{r, message=FALSE, warning=FALSE}
setwd("~/Documents/GitHub/Spring2018-Project1-xueyao0917/data/InauguralSpeeches")
speech.list$fullName <- paste(speech.list$File, speech.list$Term, sep = "-")
speech.list$fullName <- paste("inaug", speech.list$fullName, ".txt", sep = "")
speech.list$text <- NA
for(i in seq(nrow(speech.list))) {
  speech.list$text[i] <- readLines(speech.list$fullName[i])
}
speech.list[58,]$Words <- word_count(speech.list[58,]$text)
```

# 2 World Cloud
## 2.1 Text Processing
We removed extra white space, converted all letters to the lower case, removed stop words, removed empty words due to formatting errors, and removed punctuation. However, after these steps, there were still many meaningless words. Therefore, I created an array named ('mystopwords') to store them, and then removed.
```{r}
folder.path="~/Documents/GitHub/Spring2018-Project1-xueyao0917/data/InauguralSpeeches"
ff.all<-Corpus(DirSource(folder.path))
ff.all<-tm_map(ff.all, stripWhitespace)
ff.all<-tm_map(ff.all, content_transformer(tolower))
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))
ff.all<-tm_map(ff.all, removeWords, character(0))
ff.all<-tm_map(ff.all, removePunctuation)
mystopwords <- c("will","would","may","shall","can",
                 "must","ever","never","yet","every",
                 "many","one","upon","now","new",
                 "thank","make","let","great","made",
                 "know","just","back","today","across",
                 "united","states")
ff.all <- tm_map(ff.all, removeWords, mystopwords)
```

##  2.2 Word Cloud
```{r, message=FALSE, warning=FALSE, fig.height=6, fig.width=6}
par(mfrow=c(1,3))
speech.list.sorted <- speech.list[order(speech.list$File),]
ff.two <- ff.all[speech.list.sorted$twoTerm == "Y"]
ff.one <- ff.all[speech.list.sorted$twoTerm == "N"]
ff.trump <- ff.all[speech.list.sorted$twoTerm == "T"]

#Two Terms
tdm.two<-TermDocumentMatrix(ff.two)
tdm.tidy.two=tidy(tdm.two)
tdm.overall.two=summarise(group_by(tdm.tidy.two, term), sum(count))
wordcloud(tdm.overall.two$term, tdm.overall.two$`sum(count)`,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.1,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Greens"))
#One Term
tdm.one<-TermDocumentMatrix(ff.one)
tdm.tidy.one=tidy(tdm.one)
tdm.overall.one=summarise(group_by(tdm.tidy.one, term), sum(count))
wordcloud(tdm.overall.one$term, tdm.overall.one$`sum(count)`,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.1,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Reds"))

#Trump
tdm.trump<-TermDocumentMatrix(ff.trump)
tdm.tidy.trump=tidy(tdm.trump)
tdm.overall.trump=summarise(group_by(tdm.tidy.trump, term), sum(count))
wordcloud(tdm.overall.trump$term, tdm.overall.trump$`sum(count)`,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.1,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues"))
```

Both groups focused on people and government, wheile Trump focused more on America. However, there was still some differences between one-term and two-term presidents. One-term presidents usually mentioned "rights" and "country" while two-terms presidetents mentioned "peach" and "world" more. As for Trump, he adressed word "right" and "country", like a one-term president.

# 3 Sentences Analysis
We used sentences as units of analysis, as sentences were natural languge units for organizing thoughts and ideas. For each extracted sentence, we applied sentiment analysis using [NRC sentiment lexion](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm). "The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). The annotations were manually done by crowdsourcing."

## 3.1 Generate list of sentences
We assigned an sequential id to each sentence in a speech (`sent.id`) and also calculated the number of words in each sentence as sentence length (`word.count`).
```{r}
sentence.list=NULL
for(i in 1:nrow(speech.list)){
  sentences=sent_detect(speech.list$text[i],
                        endmarks = c("?", ".", "!", "|",";"))
  if(length(sentences)>0){
    emotions=get_nrc_sentiment(sentences)
    word.count=word_count(sentences)
    emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
    sentence.list=rbind(sentence.list, 
                        cbind(speech.list[i,-ncol(speech.list)],
                              sentences=as.character(sentences), 
                              word.count,
                              emotions,
                              sent.id=1:length(sentences)
                              )
    )
  }
}
```

There were some non-sentences in raw data due to erroneous extra end-of sentence marks. Removed them and saved sentence.list in the output folder.
```{r}
sentence.list=
  sentence.list%>%
  filter(!is.na(word.count))
write.csv(sentence.list,"../output/sentenceList.csv")
#sentence.list<- read.csv("~/Documents/GitHub/Spring2018-Project1-xueyao0917/output/sentenceList.csv")
```

## 3.2 Length of Sentences
```{r, fig.height=3, fig.width=3}
par(mfrow=c(3,1))
#Two Terms
sentence.list.two=sentence.list%>%filter(twoTerm=="Y", Term==1)
sentence.list.two$File=factor(sentence.list.two$File)
sentence.list.two$FileOrdered=reorder(sentence.list.two$File, 
                                  sentence.list.two$word.count, 
                                  mean, 
                                  order=T)
beeswarm(word.count~FileOrdered, 
         data=sentence.list.two,
         horizontal = TRUE,
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.5, cex.axis=0.5, cex.lab=0.8,
         spacing=5/nlevels(sentence.list.two$FileOrdered),
         las=2, ylab="", xlab="Number of words in a sentence.",
         main="Length of Sentences for Two Terms")

#One Term
sentence.list.one=sentence.list%>%filter(twoTerm=="N")
sentence.list.one$File=factor(sentence.list.one$File)
sentence.list.one$FileOrdered=reorder(sentence.list.one$File, 
                                  sentence.list.one$word.count, 
                                  mean, 
                                  order=T)
beeswarm(word.count~FileOrdered, 
         data=sentence.list.one,
         horizontal = TRUE,
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.5, cex.axis=0.5, cex.lab=0.8,
         spacing=5/nlevels(sentence.list.one$FileOrdered),
         las=2, ylab="", xlab="Number of words in a sentence.",
         main="Length of Sentences for One Term")

#Trump
sentence.list.trump=sentence.list%>%filter(twoTerm=="T")
sentence.list.trump$File=factor(sentence.list.trump$File)
sentence.list.trump$FileOrdered=reorder(sentence.list.trump$File, 
                                  sentence.list.trump$word.count, 
                                  mean, 
                                  order=T)
beeswarm(word.count~FileOrdered, 
         data=sentence.list.trump,
         horizontal = TRUE,
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.5, cex.axis=0.5, cex.lab=0.8,
         spacing=5/nlevels(sentence.list.trump$FileOrdered),
         las=2, ylab="", xlab="Number of words in a sentence.",
         main="Length of Sentences for Trump")
```

We only used the first inaugural speech of each president, to get the similar number of speeched in each group. From the first two plots, the distributions of number of words in a senetence wer close, and each sentences were usually had 10 to 30 words. Therefore, there was not obviously differences between two groups in length of sentences.

# 4 Sentiment Analsis
## 4.1 Sentence length variation over the course of the speech, with emotions. 
The presidents usually alternated between long and short sentences and shifted between different sentiments in their speeches. We used different colors to reflect different emotions. 
```{r, fig.height=3, fig.width=3}
par(mfrow=c(3,1))
f.plotsent.len(In.list=sentence.list, IntwoTerm="Y")
f.plotsent.len(In.list=sentence.list, IntwoTerm="N")
f.plotsent.len(In.list=sentence.list, IntwoTerm="T" )
```

## 4.2 Clustering of Emotions
### 4.2.1 Heatmap
```{r}
heatmap.2(cor(sentence.list%>%filter(twoTerm=="Y")%>%select(anger:trust)), 
          scale = "none", 
          col = bluered(100), , margin=c(6, 6), key=F,
          trace = "none", density.info = "none",
          main = "Heatmap for Two Terms")

heatmap.2(cor(sentence.list%>%filter(twoTerm=="N")%>%select(anger:trust)), 
          scale = "none", 
          col = bluered(100), , margin=c(6, 6), key=F,
          trace = "none", density.info = "none",
          main = "Heatmap for One Term")

heatmap.2(cor(sentence.list%>%filter(twoTerm=="T")%>%select(anger:trust)), 
          scale = "none", 
          col = bluered(100), , margin=c(6, 6), key=F,
          trace = "none", density.info = "none",
          main = "Heatmap for Trump")
```

### 4.2.2 Barplot
```{r}
par(mfrow=c(1,3))
emo.means=colMeans(select(sentence.list%>%filter(twoTerm=="Y"), anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1", 
            "chartreuse3", "blueviolet",
            "darkgoldenrod2", "dodgerblue3", 
            "darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Two Terms")

emo.means=colMeans(select(sentence.list%>%filter(twoTerm=="N"), anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1", 
            "chartreuse3", "blueviolet",
            "darkgoldenrod2", "dodgerblue3", 
            "darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="One Term")

emo.means=colMeans(select(sentence.list%>%filter(twoTerm=="T"), anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1", 
            "chartreuse3", "blueviolet",
            "darkgoldenrod2", "dodgerblue3", 
            "darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Trump")
```

Both two groups had more anticipation than joy and more sadness than surprise. However, Trump had more joy than anticipation and more surprise than sadness.

### 4.2.3 Clustering
```{r, fig.height=3, fig.width=3}
presid.summary=tbl_df(sentence.list)%>%
  group_by(File)%>%
  summarise(
    anger=mean(anger),
    anticipation=mean(anticipation),
    disgust=mean(disgust),
    fear=mean(fear),
    joy=mean(joy),
    sadness=mean(sadness),
    surprise=mean(surprise),
    trust=mean(trust)
    #negative=mean(negative),
    #positive=mean(positive)
  )

presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(presid.summary[,-1], iter.max=200, 5)
fviz_cluster(km.res, 
             stand=F, repel= TRUE,
             data = presid.summary[,-1], xlab="", xaxt="n",
             show.clust.cent=FALSE)
```

# 5 Topic modeling
## 5.1 Prepare
For topic modeling, we prepared a corpus of sentence snipets as follows. For each speech, we started with sentences and prepared a snipet with a given sentence with the flanking sentences. 
```{r}
corpus.list=sentence.list[2:(nrow(sentence.list)-1), ]
sentence.pre=sentence.list$sentences[1:(nrow(sentence.list)-2)]
sentence.post=sentence.list$sentences[3:(nrow(sentence.list)-1)]
corpus.list$snipets=paste(sentence.pre, corpus.list$sentences, sentence.post, sep=" ")
rm.rows=(1:nrow(corpus.list))[corpus.list$sent.id==1]
rm.rows=c(rm.rows, rm.rows-1)
corpus.list=corpus.list[-rm.rows, ]

docs <- Corpus(VectorSource(corpus.list$snipets))
docs <-tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs,stemDocument)
```

Build document-term matrices. 
```{r}
dtm <- DocumentTermMatrix(docs)
#Convert rownames to filenames
rownames(dtm) <- paste(corpus.list$type, corpus.list$File,
                       corpus.list$Term, corpus.list$sent.id, sep="_")

rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document

dtm  <- dtm[rowTotals> 0, ]
corpus.list=corpus.list[rowTotals>0, ]
```

## 5.2 LDA
```{r}
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE

#Number of topics
k <- 15

#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter = iter, 
                                                 thin=thin))
#Docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
write.csv(ldaOut.topics,file="../output/DocsToTopics.csv")

#Top 6 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,20))
write.csv(ldaOut.terms,file="../output/TopicsToTerms.csv")

#Probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file="../output/TopicProbabilities.csv")
```

```{r}
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
  topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
head(ldaOut.terms,5)
```

## 5.3 Clustering
```{r, message=FALSE, warning=FALSE, fig.height=3, fig.width=3}
topics.hash=c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O")
corpus.list$ldatopic=as.vector(ldaOut.topics)
corpus.list$ldahash=topics.hash[ldaOut.topics]

colnames(topicProbabilities)=topics.hash
corpus.list.df=cbind(corpus.list, topicProbabilities)

presid.summary=tbl_df(corpus.list.df)%>%
  select(File, A:O)%>%
  group_by(File)%>%
  summarise_each(funs(mean))

presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(scale(presid.summary[,-1]), iter.max=200, 5)
fviz_cluster(km.res, 
             stand=T, repel= TRUE,
             data = presid.summary[,-1],
             show.clust.cent=FALSE)
```

# Conclusion