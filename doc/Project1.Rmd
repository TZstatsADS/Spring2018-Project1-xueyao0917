---
title: "Project 1: Will Donald Trump Serve a Second Term?"
output: html_notebook
---

# Introduction
In this project, I explored whether there is a relationship between presidents who served two terms and their inaugural speeches. I compared one-term and two-term presidents' inaugural speeches in the use of words, length of sentences, emotions and topics. In addition, I analyzed the similarities and differences between Trump and these two groups and try to predict that will Donald Trump serve a second term?

# 0 Load the packages and functions
```{r, message=FALSE, warning=FALSE}
packages.used=c("rvest", "tibble", "qdap",
                "sentimentr", "gplots", "dplyr",
                "syuzhet", "factoextra","beeswarm",
                "scales", "RColorBrewer","RANN",
                "tm", "topicmodels","wordcloud",
                "tidytext","readxl")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

# packages
library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("topicmodels")
library("wordcloud")
library("tidytext")
library("readxl")

# functions
source("~/Documents/GitHub/Spring2018-Project1-xueyao0917/lib/speechFuncs.R")
```

# 1 Data Harvest
## 1.1 Import speech list
I imported the speech list and stored these information in `speech.list`. Except Trump, I divided presidents into two groups. There were 17 presidents who served two or more terms and 21 presidents who only served one term. Here was the list of two-term presidents.
```{r}
speech.list <- read_xlsx("~/Documents/GitHub/Spring2018-Project1-xueyao0917/data/InaugurationInfo.xlsx")
speech.list$twoTerm <- "N"
twoTermPresident <- speech.list[speech.list$Term == 2,]$President
print(twoTermPresident)
speech.list[is.element(speech.list$President,twoTermPresident),]$twoTerm <- "Y"
speech.list[25,]$twoTerm <- "Y" #Grover Cleveland - I
speech.list[58,]$twoTerm <- "T" #Trump
table(speech.list$twoTerm)
```

## 1.2 Import speech text
I imported the full text of speeches and created a new column named `text` to store them.
```{r, message=FALSE, warning=FALSE}
setwd("~/Documents/GitHub/Spring2018-Project1-xueyao0917/data/InauguralSpeeches")
speech.list$fullName <- paste(speech.list$File, speech.list$Term, sep = "-")
speech.list$fullName <- paste("inaug", speech.list$fullName, ".txt", sep = "")
speech.list$text <- NA
for(i in seq(nrow(speech.list))) {
  speech.list$text[i] <- readLines(speech.list$fullName[i])
}
speech.list[58,]$Words <- word_count(speech.list[58,]$text)
```

# 2 Word Cloud
Firstly, I analyzed the use of words. To do this, I created the word cloud to show the high-frequency words mentioned in inaugural speeches.

## 2.1 Text Processing
For text processing, I removed extra white space, converted all letters to the lower case, removed stop words, removed empty words due to formatting errors, and removed punctuation. However, after these steps, there were still many meaningless words. Therefore, I created an array named `mystopwords` to collect these words and removed them later.
```{r}
folder.path="~/Documents/GitHub/Spring2018-Project1-xueyao0917/data/InauguralSpeeches"
ff.all<-Corpus(DirSource(folder.path))
ff.all<-tm_map(ff.all, stripWhitespace)
ff.all<-tm_map(ff.all, content_transformer(tolower))
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))
ff.all<-tm_map(ff.all, removeWords, character(0))
ff.all<-tm_map(ff.all, removePunctuation)
mystopwords <- c("will","would","may","shall","can",
                 "must","ever","never","yet","every",
                 "many","one","upon","now","new",
                 "thank","make","let","great","made",
                 "know","just","back","today","across",
                 "united","states")
ff.all <- tm_map(ff.all, removeWords, mystopwords)
```

## 2.2 Word Cloud
I divided the speeches into three groups, 36 speeches from two-term presidents, 21 speeches from one-term presidents and 1 from Trump.
```{r, message=FALSE, warning=FALSE, fig.height=3, fig.width=8}
par(mfrow=c(1,3))
speech.list.sorted <- speech.list[order(speech.list$File),]
ff.two <- ff.all[speech.list.sorted$twoTerm == "Y"]
ff.one <- ff.all[speech.list.sorted$twoTerm == "N"]
ff.trump <- ff.all[speech.list.sorted$twoTerm == "T"]

#Two Terms
tdm.two<-TermDocumentMatrix(ff.two)
tdm.tidy.two=tidy(tdm.two)
tdm.overall.two=summarise(group_by(tdm.tidy.two, term), sum(count))
wordcloud(tdm.overall.two$term, tdm.overall.two$`sum(count)`,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.1,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Greens"))
#One Term
tdm.one<-TermDocumentMatrix(ff.one)
tdm.tidy.one=tidy(tdm.one)
tdm.overall.one=summarise(group_by(tdm.tidy.one, term), sum(count))
wordcloud(tdm.overall.one$term, tdm.overall.one$`sum(count)`,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.1,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Reds"))

#Trump
tdm.trump<-TermDocumentMatrix(ff.trump)
tdm.tidy.trump=tidy(tdm.trump)
tdm.overall.trump=summarise(group_by(tdm.tidy.trump, term), sum(count))
wordcloud(tdm.overall.trump$term, tdm.overall.trump$`sum(count)`,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.1,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues"))
```

From the word cloud, it was obvious that both two-term and one-term presidents focused on "people" and "government" most. However, there were still some differences between one-term and two-term presidents.  Two-term presidents usually mentioned "world" and "peace" while one-term presidents mentioned "country" and "rights" more.

As for Trump, he was special. Like his slogan *Make America Great Again*, he focused more on "America" and hardly mentioned "government". In this aspect, Trump was more concerned about "country" and "protected", like a one-term president.

# 3 Length of Sentences
Secondly, I used sentences as units of analysis, as sentences were natural language units for organizing thoughts and ideas. In this part, I focused on the length of sentences.

## 3.1 Generate list of sentences
I assigned a sentence id to each sentence in a speech as `sent.id` and calculated the number of words in each sentence as `word.count`.
```{r}
sentence.list=NULL
for(i in 1:nrow(speech.list)){
  sentences=sent_detect(speech.list$text[i],
                        endmarks = c("?", ".", "!", "|",";"))
  if(length(sentences)>0){
    emotions=get_nrc_sentiment(sentences)
    word.count=word_count(sentences)
    emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
    sentence.list=rbind(sentence.list, 
                        cbind(speech.list[i,-ncol(speech.list)],
                              sentences=as.character(sentences), 
                              word.count,
                              emotions,
                              sent.id=1:length(sentences)
                              )
    )
  }
}
```

There were some non-sentences in raw data due to erroneous extra end-of sentence marks. I removed them and exported sentence.list to a csv file.
```{r}
sentence.list=
  sentence.list%>%
  filter(!is.na(word.count))
write.csv(sentence.list,"../output/sentenceList.csv")
```

## 3.2 Length of Sentences
To get the similar number of speeches in each group, I only used the first inaugural speech of each president. Therefore, there were 17 speeches from two-term presidents, 21 speeches from one-term presidents and 1 from Trump. 
```{r, fig.height=3, fig.width=3}
par(mfrow=c(3,1))
#Two Terms
sentence.list.two=sentence.list%>%filter(twoTerm=="Y", Term==1)
sentence.list.two$File=factor(sentence.list.two$File)
sentence.list.two$FileOrdered=reorder(sentence.list.two$File, 
                                  sentence.list.two$word.count, 
                                  mean, 
                                  order=T)
beeswarm(word.count~FileOrdered, 
         data=sentence.list.two,
         horizontal = TRUE,
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.5, cex.axis=0.5, cex.lab=0.8,
         spacing=5/nlevels(sentence.list.two$FileOrdered),
         las=2, ylab="", xlab="Number of words in a sentence.",
         main="Length of Sentences for Two Terms")

#One Term
sentence.list.one=sentence.list%>%filter(twoTerm=="N")
sentence.list.one$File=factor(sentence.list.one$File)
sentence.list.one$FileOrdered=reorder(sentence.list.one$File, 
                                  sentence.list.one$word.count, 
                                  mean, 
                                  order=T)
beeswarm(word.count~FileOrdered, 
         data=sentence.list.one,
         horizontal = TRUE,
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.5, cex.axis=0.5, cex.lab=0.8,
         spacing=5/nlevels(sentence.list.one$FileOrdered),
         las=2, ylab="", xlab="Number of words in a sentence.",
         main="Length of Sentences for One Term")

#Trump
sentence.list.trump=sentence.list%>%filter(twoTerm=="T")
sentence.list.trump$File=factor(sentence.list.trump$File)
sentence.list.trump$FileOrdered=reorder(sentence.list.trump$File, 
                                  sentence.list.trump$word.count, 
                                  mean, 
                                  order=T)
beeswarm(word.count~FileOrdered, 
         data=sentence.list.trump,
         horizontal = TRUE,
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.5, cex.axis=0.5, cex.lab=0.8,
         spacing=5/nlevels(sentence.list.trump$FileOrdered),
         las=2, ylab="", xlab="Number of words in a sentence.",
         main="Length of Sentences for Trump")
```

From the first two plots, the distributions of number of words in a sentence were close. Most sentences had 10 to 30 words, while one-term presidents tended to have relatively longer sentences. In this aspect, Trump was special again. The length of his sentences was around 5 to 20 and he did not use any long sentence.

# 4 Sentiment Analysis
Then, for each extracted sentence, I applied sentiment analysis using [NRC sentiment lexion](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm). "The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). The annotations were manually done by crowdsourcing."

## 4.1 Sentence length variation with emotions
The presidents usually alternated between long and short sentences and shifted between different sentiments in their speeches.
```{r, fig.height=3, fig.width=3}
par(mfrow=c(3,1))
f.plotsent.len(In.list=sentence.list, IntwoTerm="Y")
f.plotsent.len(In.list=sentence.list, IntwoTerm="N")
f.plotsent.len(In.list=sentence.list, IntwoTerm="T" )
```

From the plot, although one-term presidents had less number of speeches, they still were more colorful. It indicated that one-term presidents had stronger emotions than two-term presidents. In this aspect, emotional Trump had a great probability to be a one-term president.

## 4.2 Clustering of Emotions
### 4.2.1 Heatmap
```{r, fig.height=2, fig.width=3}
heatmap.2(cor(sentence.list%>%filter(twoTerm=="Y")%>%select(anger:trust)), 
          scale = "none", 
          col = bluered(100), , margin=c(6, 6), key=F,
          trace = "none", density.info = "none",
          main = "Heatmap for Two Terms")

heatmap.2(cor(sentence.list%>%filter(twoTerm=="N")%>%select(anger:trust)), 
          scale = "none", 
          col = bluered(100), , margin=c(6, 6), key=F,
          trace = "none", density.info = "none",
          main = "Heatmap for One Term")

heatmap.2(cor(sentence.list%>%filter(twoTerm=="T")%>%select(anger:trust)), 
          scale = "none", 
          col = bluered(100), , margin=c(6, 6), key=F,
          trace = "none", density.info = "none",
          main = "Heatmap for Trump")
```

From the heatmaps, emotions were all divided into two parts based on the correlation. One part was anger, fear, disgust and sadness, and the other part was anticipation, surprise, joy and trust. The heatmaps of one-term and two-term presidents were similar. 

However, Trump had strong positive emotions of trust, joy and anticipation. These emotions were highly related to anger. This was quite different from other presidents.

### 4.2.2 Barplot
```{r, fig.height=3, fig.width=8}
par(mfrow=c(1,3))
emo.means=colMeans(select(sentence.list%>%filter(twoTerm=="Y"), anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1", 
            "chartreuse3", "blueviolet",
            "darkgoldenrod2", "dodgerblue3", 
            "darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means[order(emo.means)], las=2, cex.axis=1.5, cex.names=1.5,
        col=col.use[order(emo.means)], horiz=T, main="Two Terms")

emo.means=colMeans(select(sentence.list%>%filter(twoTerm=="N"), anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1", 
            "chartreuse3", "blueviolet",
            "darkgoldenrod2", "dodgerblue3", 
            "darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means[order(emo.means)], las=2, cex.axis=1.5, cex.names=1.5,
        col=col.use[order(emo.means)], horiz=T, main="One Term")

emo.means=colMeans(select(sentence.list%>%filter(twoTerm=="T"), anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1", 
            "chartreuse3", "blueviolet",
            "darkgoldenrod2", "dodgerblue3", 
            "darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means[order(emo.means)], las=2, cex.axis=1.5, cex.names=1.5,
        col=col.use[order(emo.means)], horiz=T, main="Trump")
```

From the barplots, both two groups had more anticipation than joy and more sadness than surprise. However, Trump had more joy than anticipation and more surprise than sadness. Furthermore, comparing to other presidents, Trump was more positive and had less emotions of fear, anger, sadness and disgust.

### 4.2.3 Cluster plot
```{r, fig.height=2, fig.width=3}
presid.summary=tbl_df(sentence.list)%>%
  group_by(File)%>%
  summarise(
    anger=mean(anger),
    anticipation=mean(anticipation),
    disgust=mean(disgust),
    fear=mean(fear),
    joy=mean(joy),
    sadness=mean(sadness),
    surprise=mean(surprise),
    trust=mean(trust)
    #negative=mean(negative),
    #positive=mean(positive)
  )

presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(presid.summary[,-1], iter.max=200, 5)
fviz_cluster(km.res, 
             stand=F, repel= TRUE,
             data = presid.summary[,-1], xlab="", xaxt="n",
             show.clust.cent=FALSE)
```

From the cluster plot, Trump was in the part where one-term presidents were more than two-term presidents.

# 5 Topic modeling
## 5.1 LDA
For topic modeling, I processed the text and built the document-term matrices. Then I fitted an LDA model with 15 topics. 
```{r}
corpus.list=sentence.list[2:(nrow(sentence.list)-1), ]
sentence.pre=sentence.list$sentences[1:(nrow(sentence.list)-2)]
sentence.post=sentence.list$sentences[3:(nrow(sentence.list)-1)]
corpus.list$snipets=paste(sentence.pre, corpus.list$sentences, sentence.post, sep=" ")
rm.rows=(1:nrow(corpus.list))[corpus.list$sent.id==1]
rm.rows=c(rm.rows, rm.rows-1)
corpus.list=corpus.list[-rm.rows, ]

docs <- Corpus(VectorSource(corpus.list$snipets))
docs <-tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs,stemDocument)

dtm <- DocumentTermMatrix(docs)
#Convert rownames to filenames
rownames(dtm) <- paste(corpus.list$type, corpus.list$File,
                       corpus.list$Term, corpus.list$sent.id, sep="_")

rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document

dtm  <- dtm[rowTotals> 0, ]
corpus.list=corpus.list[rowTotals>0, ]
```

```{r}
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE

#Number of topics
k <- 15

#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter = iter, 
                                                 thin=thin))
#Docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
write.csv(ldaOut.topics,file="../output/DocsToTopics.csv")

#Top 6 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,20))
write.csv(ldaOut.terms,file="../output/TopicsToTerms.csv")

#Probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file="../output/TopicProbabilities.csv")
```

The 15 topics were listed as follows.
```{r}
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
  topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
head(ldaOut.terms,5)
```

## 5.2 Cluster plot
```{r, message=FALSE, warning=FALSE, fig.height=2, fig.width=3}
topics.hash=c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O")
corpus.list$ldatopic=as.vector(ldaOut.topics)
corpus.list$ldahash=topics.hash[ldaOut.topics]

colnames(topicProbabilities)=topics.hash
corpus.list.df=cbind(corpus.list, topicProbabilities)

presid.summary=tbl_df(corpus.list.df)%>%
  select(File, A:O)%>%
  group_by(File)%>%
  summarise_each(funs(mean))

presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(scale(presid.summary[,-1]), iter.max=200, 5)
fviz_cluster(km.res, 
             stand=T, repel= TRUE,
             data = presid.summary[,-1],
             show.clust.cent=FALSE)
```

From the cluster plot, Trump was in the part that most presidents only had one term.

# Conclusion
In this project, I discussed the differences between one-term and two-term presidents in the use of words, length of sentences, emotions and topics. And then, I analyzed the similarities and differences between Trump and these two groups. 

In conclusion, Trump is more likely to be a one-term president for his use of words, emotions and topics. However, Trump is a really special president in the United States, like his short sentences and strong positive sentiment. Therefore, it is hard to make a prediction based on the analysis of inaugural speeches.
